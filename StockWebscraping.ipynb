{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "fde21970",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A notebook that gets daily closing prices, calculates log returns, alpha, beta, and Sharpe Ratio\n",
    "#TODO: Scrape earnings reports so we have some more graphs to work with\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import twint\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "25ef76db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataFrame(stockList, startDate, endDate):\n",
    "    stockDataFrame = pd.DataFrame(columns = stockList)\n",
    "    for stock in stockList:\n",
    "        stockDataFrame[stock] = yf.download(stock, start=startDate, end=endDate, progress=False)['Close']\n",
    "    return stockDataFrame\n",
    "\n",
    "def logReturns(stockDataFrame):\n",
    "    for stock in stockDataFrame.columns:\n",
    "        stockDataFrame[stock] = np.log(stockDataFrame[stock]) - np.log(stockDataFrame[stock].shift(1))\n",
    "    stockDataFrame.dropna(inplace=True)\n",
    "    return stockDataFrame\n",
    "\n",
    "#We use rolling alpha, beta in this case. Comparing with the SP500 for linear regression\n",
    "def marketAlphaBeta(logReturnDF, benchmarkDF):\n",
    "    alphaDataFrame = pd.DataFrame(columns = logReturnDF.columns, index=logReturnDF.index)\n",
    "    betaDataFrame = pd.DataFrame(columns = logReturnDF.columns, index=logReturnDF.index)\n",
    "    obs = logReturnDF.shape[0]\n",
    "    lagWindow = 30\n",
    "    for i in range((obs-lagWindow)):\n",
    "        for stock in logReturnDF.columns:\n",
    "            regressor = LinearRegression()\n",
    "            regressor.fit(benchmarkDF['^GSPC'].to_numpy()[i : i +lagWindow+1].reshape(-1,1), logReturnDF[stock].to_numpy()[i : i +lagWindow+1])\n",
    "            betaDataFrame[stock][i+lagWindow]  = regressor.coef_[0]\n",
    "            alphaDataFrame[stock][i+lagWindow]  = regressor.intercept_\n",
    "    alphaDataFrame.dropna(inplace=True)\n",
    "    betaDataFrame.dropna(inplace=True)\n",
    "    return alphaDataFrame, betaDataFrame\n",
    "\n",
    "#We use rolling Sharpe ratio in this case. We use 10 year Treasury Note (^TNX) yield as \"risk-free\" rate\n",
    "def rollingSharpeRatio(logReturnDF, logBenchmark):\n",
    "    sharpeDataFrame = pd.DataFrame(columns = logReturnDF.columns, index = logReturnDF.index)\n",
    "    obs = logReturnDF.shape[0]\n",
    "    lagWindow = 60\n",
    "    for i in range((obs-lagWindow)):\n",
    "        for stock in logReturnDF.columns:\n",
    "            netReturn = logReturnDF[stock][i : i +lagWindow+1].mean() - logBenchmark['^TNX'][i : i +lagWindow+1].mean()\n",
    "            stdDev = logReturnDF[stock][i : i +lagWindow+1].std()\n",
    "            sharpeDataFrame[stock][i+lagWindow]  = netReturn/stdDev\n",
    "    sharpeDataFrame.dropna(inplace=True)\n",
    "    return sharpeDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "76629b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables that we can modify to get our data\n",
    "memeStocks = ['AAPL','GOOG','TSLA','KO','OXY','BAC']\n",
    "benchmarks = ['^GSPC','^TNX']\n",
    "startDate = '2022-3-24'\n",
    "endDate = '2023-3-24'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "e26ff80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run once variables are filled out\n",
    "newFrame = createDataFrame(memeStocks, startDate, endDate)\n",
    "newFrame.to_csv(\"stockPrices.csv\")\n",
    "benchmarkFrame = createDataFrame(benchmarks, startDate, endDate)\n",
    "benchmarkFrame.to_csv(\"benchmarkPrices.csv\")\n",
    "logDataFrame = logReturns(newFrame)\n",
    "logDataFrame.to_csv(\"logReturnsStock.csv\")\n",
    "logBenchmark = logReturns(benchmarkFrame)\n",
    "logBenchmark.to_csv(\"logReturnsBenchmark.csv\")\n",
    "alphaFrame, betaFrame = marketAlphaBeta(logDataFrame, logBenchmark)\n",
    "alphaFrame.to_csv(\"alphas.csv\")\n",
    "betaFrame.to_csv(\"betas.csv\")\n",
    "sharpeDataFrame = rollingSharpeRatio(logDataFrame, logBenchmark)\n",
    "sharpeDataFrame.to_csv(\"sharpeRatios.csv\")\n",
    "newFrame = createDataFrame(memeStocks, startDate, endDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "6baf7fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(newFrame, benchmarkFrame, left_index=True, right_index=True)\n",
    "df['Date'] = df.index\n",
    "df = pd.melt(df, id_vars=[i for i in df.columns.values if i not in memeStocks], value_vars=memeStocks, var_name='Ticker', value_name='Price')\n",
    "logDataFrame['Date'] = logDataFrame.index\n",
    "log_melted = pd.melt(logDataFrame, id_vars=['Date'],value_vars=memeStocks, var_name='Ticker', value_name='Price')\n",
    "df = pd.merge(df, log_melted, on=['Ticker','Date'], how='inner', suffixes=['_Stock','_Log_Return'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "c0a926f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "def scrapeForDate(ticker,dates):\n",
    "    df = pd.DataFrame()\n",
    "    since = processDateRange(dates.shift(freq='-1D'))\n",
    "    until = processDateRange(dates)\n",
    "    for j,day in enumerate(since):\n",
    "        tweets_list = []\n",
    "        query_str = f'${ticker} lang:en since:{since[j]} until:{until[j]}'\n",
    "        for i,tweet in enumerate(sntwitter.TwitterSearchScraper(query_str).get_items()):\n",
    "            if i>=60:\n",
    "                break\n",
    "            tweets_list.append([tweet.date, tweet.id, tweet.content])\n",
    "        temp = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text'])\n",
    "        df = pd.concat([df,temp])\n",
    "    return df\n",
    "\n",
    "def processDateRange(date_range):\n",
    "    dates = [datetime.strptime(str(date), '%Y-%m-%d %H:%M:%S') for date in date_range.strftime('%Y-%m-%d %H:%M:%S')]\n",
    "    dates = [date.strftime('%Y-%m-%d') for date in dates]\n",
    "    return dates    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "96b73b22",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[237], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m warnings\u001b[39m.\u001b[39msimplefilter(\u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m ticker \u001b[39min\u001b[39;00m memeStocks:\n\u001b[0;32m----> 9\u001b[0m     df \u001b[39m=\u001b[39m scrapeForDate(ticker, date_range)\n\u001b[1;32m     10\u001b[0m     dfs\u001b[39m.\u001b[39mappend(df)\n",
      "Cell \u001b[0;32mIn[236], line 9\u001b[0m, in \u001b[0;36mscrapeForDate\u001b[0;34m(ticker, dates)\u001b[0m\n\u001b[1;32m      7\u001b[0m tweets_list \u001b[39m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m query_str \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m$\u001b[39m\u001b[39m{\u001b[39;00mticker\u001b[39m}\u001b[39;00m\u001b[39m lang:en since:\u001b[39m\u001b[39m{\u001b[39;00msince[j]\u001b[39m}\u001b[39;00m\u001b[39m until:\u001b[39m\u001b[39m{\u001b[39;00muntil[j]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfor\u001b[39;00m i,tweet \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(sntwitter\u001b[39m.\u001b[39mTwitterSearchScraper(query_str)\u001b[39m.\u001b[39mget_items()):\n\u001b[1;32m     10\u001b[0m     \u001b[39mif\u001b[39;00m i\u001b[39m>\u001b[39m\u001b[39m=\u001b[39m\u001b[39m60\u001b[39m:\n\u001b[1;32m     11\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/snscrape/modules/twitter.py:1661\u001b[0m, in \u001b[0;36mTwitterSearchScraper.get_items\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1658\u001b[0m params \u001b[39m=\u001b[39m paginationParams\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m   1659\u001b[0m \u001b[39mdel\u001b[39;00m params[\u001b[39m'\u001b[39m\u001b[39mcursor\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m-> 1661\u001b[0m \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter_api_data(\u001b[39m'\u001b[39m\u001b[39mhttps://api.twitter.com/2/search/adaptive.json\u001b[39m\u001b[39m'\u001b[39m, _TwitterAPIType\u001b[39m.\u001b[39mV2, params, paginationParams, cursor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cursor):\n\u001b[1;32m   1662\u001b[0m \t\u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_v2_timeline_instructions_to_tweets_or_users(obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/snscrape/modules/twitter.py:761\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._iter_api_data\u001b[0;34m(self, endpoint, apiType, params, paginationParams, cursor, direction)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    760\u001b[0m \t_logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mRetrieving scroll page \u001b[39m\u001b[39m{\u001b[39;00mcursor\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 761\u001b[0m \tobj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_api_data(endpoint, apiType, reqParams)\n\u001b[1;32m    762\u001b[0m \t\u001b[39myield\u001b[39;00m obj\n\u001b[1;32m    764\u001b[0m \t\u001b[39m# No data format test, just a hard and loud crash if anything's wrong :-)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/snscrape/modules/twitter.py:727\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._get_api_data\u001b[0;34m(self, endpoint, apiType, params)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[39mif\u001b[39;00m apiType \u001b[39mis\u001b[39;00m _TwitterAPIType\u001b[39m.\u001b[39mGRAPHQL:\n\u001b[1;32m    726\u001b[0m \tparams \u001b[39m=\u001b[39m urllib\u001b[39m.\u001b[39mparse\u001b[39m.\u001b[39murlencode({k: json\u001b[39m.\u001b[39mdumps(v, separators \u001b[39m=\u001b[39m (\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m)) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m params\u001b[39m.\u001b[39mitems()}, quote_via \u001b[39m=\u001b[39m urllib\u001b[39m.\u001b[39mparse\u001b[39m.\u001b[39mquote)\n\u001b[0;32m--> 727\u001b[0m r \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get(endpoint, params \u001b[39m=\u001b[39;49m params, headers \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apiHeaders, responseOkCallback \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_api_response)\n\u001b[1;32m    728\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    729\u001b[0m \tobj \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/snscrape/base.py:251\u001b[0m, in \u001b[0;36mScraper._get\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 251\u001b[0m \t\u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\u001b[39m'\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/snscrape/base.py:203\u001b[0m, in \u001b[0;36mScraper._request\u001b[0;34m(self, method, url, params, data, headers, timeout, responseOkCallback, allowRedirects, proxies)\u001b[0m\n\u001b[1;32m    201\u001b[0m \t_logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m... with environmentSettings: \u001b[39m\u001b[39m{\u001b[39;00menvironmentSettings\u001b[39m!r}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    202\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m \tr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session\u001b[39m.\u001b[39;49msend(req, allow_redirects \u001b[39m=\u001b[39;49m allowRedirects, timeout \u001b[39m=\u001b[39;49m timeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49menvironmentSettings)\n\u001b[1;32m    204\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mRequestException \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    205\u001b[0m \t\u001b[39mif\u001b[39;00m attempt \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retries:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/http/client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1373\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1375\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "dfs = []\n",
    "date_range = pd.date_range(start=startDate, end=endDate)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    for ticker in memeStocks:\n",
    "        df = scrapeForDate(ticker, date_range)\n",
    "        dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f0be2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,df in enumerate(dfs):\n",
    "    df['Ticker'] = memeStocks[i]\n",
    "    df.to_csv(f'Data/Tweets_new/{memeStocks[i]}_Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29842f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import os\n",
    "files = [os.path.join(f'{os.getcwd()}/Data/Tweets_new',path) for path in os.listdir('Data/Tweets')]\n",
    "tweets = [pd.read_csv(file,index_col=0,engine='python') for file in files]\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for i in range(len(tweets)):\n",
    "    tweets[i].dropna(inplace=True)\n",
    "    scores = []\n",
    "    for j, row in tweets[i].iterrows():\n",
    "        vs = analyzer.polarity_scores(row['Text'])['compound']\n",
    "        scores.append(vs)\n",
    "    tweets[i]['polarity_score'] = scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71052557",
   "metadata": {},
   "source": [
    "**Score Interpretation** <br>\n",
    "positive sentiment: compound score >= 0.05 <br>\n",
    "neutral sentiment: (compound score > -0.05) and (compound score < 0.05) <br>\n",
    "negative sentiment: compound score <= -0.05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcfe74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>^GSPC</th>\n",
       "      <th>^TNX</th>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Price_Stock</th>\n",
       "      <th>Price_Log_Return</th>\n",
       "      <th>polarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005053</td>\n",
       "      <td>0.062507</td>\n",
       "      <td>2022-03-25</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>174.720001</td>\n",
       "      <td>0.003727</td>\n",
       "      <td>0.140153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007120</td>\n",
       "      <td>-0.006037</td>\n",
       "      <td>2022-03-28</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>175.600006</td>\n",
       "      <td>0.005024</td>\n",
       "      <td>0.199839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.012182</td>\n",
       "      <td>-0.031579</td>\n",
       "      <td>2022-03-29</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>178.960007</td>\n",
       "      <td>0.018954</td>\n",
       "      <td>0.220375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.006314</td>\n",
       "      <td>-0.017655</td>\n",
       "      <td>2022-03-30</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>177.770004</td>\n",
       "      <td>-0.006672</td>\n",
       "      <td>0.183917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.015776</td>\n",
       "      <td>-0.013234</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>174.610001</td>\n",
       "      <td>-0.017936</td>\n",
       "      <td>0.213395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.003404</td>\n",
       "      <td>0.021259</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>174.309998</td>\n",
       "      <td>-0.001720</td>\n",
       "      <td>0.185623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.008058</td>\n",
       "      <td>0.014617</td>\n",
       "      <td>2022-04-04</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>178.440002</td>\n",
       "      <td>0.023417</td>\n",
       "      <td>0.247065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.012631</td>\n",
       "      <td>0.057987</td>\n",
       "      <td>2022-04-05</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>175.059998</td>\n",
       "      <td>-0.019124</td>\n",
       "      <td>0.205817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.009764</td>\n",
       "      <td>0.020523</td>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>171.830002</td>\n",
       "      <td>-0.018623</td>\n",
       "      <td>0.113833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.004244</td>\n",
       "      <td>0.016347</td>\n",
       "      <td>2022-04-07</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>172.139999</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>0.247603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ^GSPC      ^TNX       Date Ticker  Price_Stock  Price_Log_Return  \\\n",
       "0  0.005053  0.062507 2022-03-25   AAPL   174.720001          0.003727   \n",
       "1  0.007120 -0.006037 2022-03-28   AAPL   175.600006          0.005024   \n",
       "2  0.012182 -0.031579 2022-03-29   AAPL   178.960007          0.018954   \n",
       "3 -0.006314 -0.017655 2022-03-30   AAPL   177.770004         -0.006672   \n",
       "4 -0.015776 -0.013234 2022-03-31   AAPL   174.610001         -0.017936   \n",
       "5  0.003404  0.021259 2022-04-01   AAPL   174.309998         -0.001720   \n",
       "6  0.008058  0.014617 2022-04-04   AAPL   178.440002          0.023417   \n",
       "7 -0.012631  0.057987 2022-04-05   AAPL   175.059998         -0.019124   \n",
       "8 -0.009764  0.020523 2022-04-06   AAPL   171.830002         -0.018623   \n",
       "9  0.004244  0.016347 2022-04-07   AAPL   172.139999          0.001802   \n",
       "\n",
       "   polarity_score  \n",
       "0        0.140153  \n",
       "1        0.199839  \n",
       "2        0.220375  \n",
       "3        0.183917  \n",
       "4        0.213395  \n",
       "5        0.185623  \n",
       "6        0.247065  \n",
       "7        0.205817  \n",
       "8        0.113833  \n",
       "9        0.247603  "
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(tweets)):\n",
    "    tweets[i]['Datetime'] = pd.to_datetime(tweets[i]['Datetime'])\n",
    "    tweets[i]['Datetime'] = tweets[i]['Datetime'].dt.date\n",
    "    tweets[i].rename(columns={\"Datetime\": \"Date\"},inplace=True)\n",
    "\n",
    "tweet_df = pd.DataFrame()\n",
    "for i in range(len(tweets)):    \n",
    "    temp=tweets[i].groupby('Date')['polarity_score'].mean().reset_index()\n",
    "    temp['Ticker']=tweets[i]['Ticker'].values[0]\n",
    "    tweet_df=pd.concat([tweet_df, temp])\n",
    "tweet_df['Date'] = pd.to_datetime(tweet_df['Date'])\n",
    "\n",
    "df = pd.merge(df, tweet_df, on=['Date','Ticker'], how='inner')\n",
    "df.to_csv('finalizedDataset.csv')\n",
    "\n",
    "df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "d53292f4fdbc407f3751602e47416b1da2b74024acf0fb5ba3434bfe364ec9bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
